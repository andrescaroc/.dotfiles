# ---- llm ----
model: ollama:qwen3:1.7b         # Specify the LLM to use
temperature: null                # Set default temperature parameter (0, 1)
top_p: null                      # Set default top-p parameter, with a range of (0, 1) or (0, 2) depending on the model


# ---- behavior ----
stream: true                     # Controls whether to use the stream-style API.
save: true                       # Indicates whether to persist the message
keybindings: null               # Choose keybinding style (emacs, vi)
editor: null                     # Specifies the command used to edit input buffer or session. (e.g. vim, emacs, nano).
wrap: no                         # Controls text wrapping (no, auto, <max-width>)
wrap_code: false                 # Enables or disables wrapping of code blocks

# ---- function-calling ----
# Visit https://github.com/sigoden/llm-functions for setup instructions
function_calling: true           # Enables or disables function calling (Globally).
mapping_tools:                   # Alias for a tool or toolset
  fs: 'fs_cat,fs_ls,fs_mkdir,fs_rm,fs_write'
use_tools: all                  # Which tools to use by default. (e.g. 'fs,web_search')



# ---- apperence ----
highlight: true                  # Controls syntax highlighting
light_theme: false               # Activates a light color theme when true. env: AICHAT_LIGHT_THEME
# Custom REPL left/right prompts, see https://github.com/sigoden/aichat/wiki/Custom-REPL-Prompt for more details
left_prompt:
  '{color.green}{?session {?agent {agent}>}{session}{?role /}}{!session {?agent {agent}>}}{role}{?rag @{rag}}{color.cyan}{?session )}{!session >}{color.reset} '
right_prompt:
  '{color.purple}{?session {?consume_tokens {consume_tokens}({consume_percent}%)}{!consume_tokens {consume_tokens}}}{color.reset}'

# ---- prelude ----
repl_prelude: null               # Set a default role or session for REPL mode (e.g. role:<name>, session:<name>, <session>:<role>)
cmd_prelude: null                # Set a default role or session for CMD mode (e.g. role:<name>, session:<name>, <session>:<role>)
agent_prelude: null              # Set a session to use when starting a agent (e.g. temp, default)


# ---- session ----
# Controls the persistence of the session. if true, auto save; if false, not save; if null, asking the user
save_session: null
# Compress session when token count reaches or exceeds this threshold
compress_threshold: 4000
# Text prompt used for creating a concise summary of session message
summarize_prompt: 'Summarize the discussion briefly in 200 words or less to use as a prompt for future context.'
# Text prompt used for including the summary of the entire session
summary_prompt: 'This is a summary of the chat history as a recap: '

 
# 1. GLOBAL RAG DEFAULTS
rag_embedding_model: ollama:nomic-embed-text:v1.5
rag_top_k: 5
rag_chunk_size: 3000
rag_chunk_overlap: 150
rag_template: |
  Answer the query based on the context while respecting the rules. (user query, some textual context and rules, all inside xml tags)

  <context>
  __CONTEXT__
  </context>

  <rules>
  - If you don't know, just say so.
  - If you are not sure, ask for clarification.
  - Answer in the same language as the user query.
  - If the context appears unreadable or of poor quality, tell the user then answer as best as you can.
  - If the answer is not in the context but you think you know the answer, explain that to the user then answer with your own knowledge.
  - Answer directly and without using xml tags.
  </rules>

  <user_query>
  __INPUT__
  </user_query>
# 2. CUSTOM DOCUMENT LOADER DEFINITIONS
document_loaders:
  # Defines our 'git:' protocol using yek and jq.
  git: >
    sh -c "yek '$1' --json | jq '[.[] | { path: .filename, contents: .content }]'"

clients:
  - type: openai-compatible
    name: ollama
    api_base: http://localhost:11434/v1
    models:
      - name: deepseek-r1:1.5b
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: deepseek-r1
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: qwen3:0.6b
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: qwen3:1.7b
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: qwen3:8b
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: cogito:3b
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: qwen2.5-coder:1.5b
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: qwen2.5-coder:7b
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: codellama:7b-instruct
        max_input_tokens: 131072
        supports_reasoning: true
        supports_function_calling: true
      - name: nomic-embed-text:v1.5
        type: embedding
        max_tokens_per_chunk: 8192
        default_chunk_size: 1000
        max_batch_size: 50
      - name: gemma3:1b
        max_input_tokens: 131072
        supports_vision: true
